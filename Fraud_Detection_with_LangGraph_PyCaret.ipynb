{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef44f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "from langchain_ollama.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7d15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    dataset_code: str\n",
    "    model_code: str\n",
    "    interpretation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04771ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"gemma3:latest\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca277ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1 - Generate Synthetic Fraud Dataset Using Faker\n",
    "\n",
    "def generate_synthetic_data(state: AgentState) -> AgentState:\n",
    "    prompt = \"\"\"\n",
    "You are a Python data engineer.\n",
    "\n",
    "Generate Python code that:\n",
    "- Uses `Faker` and `pandas` to create a synthetic fraud detection dataset\n",
    "- Contains 1000 rows with columns: name, email, transaction_amount, transaction_date, location, is_fraud\n",
    "- Randomly assigns ~5% of rows as fraud (is_fraud = 1)\n",
    "- Saves the dataset to 'synthetic_fraud.csv'\n",
    "- Is fully executable and self-contained\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"dataset_code\": response.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c03747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 2 - Build Classification Model using PyCaret\n",
    "\n",
    "def build_model_with_pycaret(state: AgentState) -> AgentState:\n",
    "    prompt = f\"\"\"\n",
    "You are a data scientist.\n",
    "\n",
    "Write end-to-end Python code using PyCaret to:\n",
    "1. Load the dataset 'synthetic_fraud.csv'\n",
    "2. Setup classification using 'is_fraud' as the target column\n",
    "3. Compare models\n",
    "4. Select the best performing model\n",
    "5. Evaluate the model and display metrics\n",
    "\n",
    "Assume the dataset was generated using:\n",
    "{state['dataset_code']}\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"model_code\": response.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488d22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 3 - Interpret Model Results\n",
    "\n",
    "def interpret_results(state: AgentState) -> AgentState:\n",
    "    prompt = f\"\"\"\n",
    "You are a machine learning analyst.\n",
    "\n",
    "Given the following PyCaret code:\n",
    "{state['model_code']}\n",
    "\n",
    "Please summarize:\n",
    "- Which model performed best?\n",
    "- What are the key performance metrics (AUC, accuracy)?\n",
    "- Any recommendations based on the results\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"interpretation\": response.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a4bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangGraph and Define Flow\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# nodes\n",
    "graph.add_node(\"generate_data\", generate_synthetic_data)\n",
    "graph.add_node(\"build_model\", build_model_with_pycaret)\n",
    "graph.add_node(\"interpret\", interpret_results)\n",
    "\n",
    "graph.set_entry_point(\"generate_data\")\n",
    "graph.add_edge(\"generate_data\", \"build_model\")\n",
    "graph.add_edge(\"build_model\", \"interpret\")\n",
    "graph.set_finish_point(\"interpret\")\n",
    "\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc217ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Provide initial user input\n",
    "user_question = \"Create a fraud detection model using synthetic data and interpret the results\"\n",
    "\n",
    "# Run the LangGraph workflow\n",
    "result = app.invoke({\"user_input\": user_question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61057d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Data Generation Code\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from faker import Faker\n",
       "\n",
       "# Initialize Faker\n",
       "fake = Faker()\n",
       "\n",
       "# Set the number of rows\n",
       "num_rows = 1000\n",
       "\n",
       "# Generate data\n",
       "data = {\n",
       "    'name': [fake.name() for _ in range(num_rows)],\n",
       "    'email': [fake.email() for _ in range(num_rows)],\n",
       "    'transaction_amount': np.random.uniform(10, 1000, num_rows),\n",
       "    'transaction_date': pd.to_datetime([''.join(fake.date_time_this_decade()) for _ in range(num_rows)]),\n",
       "    'location': [fake.city() for _ in range(num_rows)],\n",
       "    'is_fraud': np.random.choice([0, 1], size=num_rows, p=[0.95, 0.05])  # 5% fraud\n",
       "}\n",
       "\n",
       "# Create DataFrame\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "# Save to CSV\n",
       "df.to_csv('synthetic_fraud.csv', index=False)\n",
       "\n",
       "print(\"synthetic_fraud.csv created successfully!\")\n",
       "```\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "print(\"Synthetic Data Generation Code\")\n",
    "display(Markdown(f\"```python\\n{result['dataset_code']}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e75e764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyCaret Classification Code**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from pycaret.classification import *\n",
       "import time\n",
       "\n",
       "# Load the dataset 'synthetic_fraud.csv'\n",
       "df = pd.read_csv('synthetic_fraud.csv')\n",
       "\n",
       "# Print the first few rows of the dataset\n",
       "print(df.head())\n",
       "\n",
       "# Setup classification using 'is_fraud' as the target column\n",
       "s = setup(data=df, target='is_fraud', session_id=42, verbose=True)\n",
       "\n",
       "# Compare models\n",
       "time.sleep(5)  # Allow models to train - increase if needed\n",
       "best_model = compare_models(n_models=5, fold=5, sort='AUC', verbose=True)\n",
       "\n",
       "# Select the best performing model\n",
       "best_model_name = best_model.index[np.argmax(best_model['AUC'])]\n",
       "print(f\"\\nBest performing model: {best_model_name}\")\n",
       "\n",
       "# Evaluate the model and display metrics\n",
       "predictions = s.predict(predictions=best_model_name)\n",
       "predictions = pd.DataFrame({'is_fraud': predictions})\n",
       "predictions = predictions.merge(df, on='index')\n",
       "print(\"\\nPredictions:\")\n",
       "print(predictions.head())\n",
       "\n",
       "# Evaluate the model\n",
       "evaluate_model(best_model_name)\n",
       "\n",
       "# Get the best model\n",
       "best_model_name = s.retrain_model(best_model_name)\n",
       "print(f\"\\nRetrained model: {best_model_name}\")\n",
       "\n",
       "# Evaluate the retrained model\n",
       "evaluate_model(best_model_name)\n",
       "```\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"PyCaret Classification Code**\")\n",
    "display(Markdown(f\"```python\\n{result['model_code']}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e8bba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Interpretation\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, here's a summary of the PyCaret code execution and recommendations, based on the provided snippet:\n",
       "\n",
       "**Summary of Execution:**\n",
       "\n",
       "The code performs a standard PyCaret classification workflow to detect fraudulent transactions. Here's a breakdown of the steps:\n",
       "\n",
       "1.  **Data Loading & Setup:** Loads the 'synthetic_fraud.csv' dataset and sets up a classification task using 'is_fraud' as the target variable.  A session ID of 42 is used for reproducibility.\n",
       "2.  **Model Comparison:**  `compare_models` is used to train and evaluate 5 different classification models (likely Logistic Regression, XGBoost, Random Forest, etc.) using 5-fold cross-validation. The models are sorted by AUC (Area Under the ROC Curve).\n",
       "3.  **Best Model Selection:** The model with the highest AUC is identified as the `best_model_name`.\n",
       "4.  **Initial Evaluation:** The `best_model_name` is evaluated using `evaluate_model`, providing initial performance metrics.\n",
       "5.  **Retraining:** The `best_model_name` is retrained using `s.retrain_model()`.\n",
       "6.  **Retrained Evaluation:** The retrained model is evaluated again.\n",
       "\n",
       "**Key Performance Metrics (Based on the Code):**\n",
       "\n",
       "*   **Best Performing Model:** The code identifies the model with the highest AUC as the `best_model_name`.  The specific model name isn't explicitly printed, but the code indicates that it's the model with the highest AUC.\n",
       "*   **AUC:** The AUC value for the best model is used to rank the models.\n",
       "*   **Accuracy:** The `evaluate_model` function will also calculate and display the accuracy metric. However, the code snippet doesn't show the final accuracy value.\n",
       "\n",
       "**Recommendations:**\n",
       "\n",
       "1.  **Retrieve and Analyze Accuracy:** The most critical missing piece is the actual accuracy value for the best model.  You *must* examine the output of `evaluate_model(best_model_name)` to see the final accuracy score.  This is the primary metric for judging the model's performance.\n",
       "\n",
       "2.  **Consider Precision and Recall:** While AUC is a good overall measure, it's crucial to also look at precision and recall, especially in fraud detection.  A model with high AUC might have a high false positive rate (many non-fraudulent transactions flagged as fraud).  The `evaluate_model` function should provide these metrics.\n",
       "\n",
       "3.  **Increase `time.sleep()`:** The `time.sleep(5)` call is a deliberate pause to allow the models to train.  If the training process is consistently slow, you might need to increase this duration.\n",
       "\n",
       "4.  **Further Exploration:**  After obtaining the accuracy, precision, and recall, consider:\n",
       "    *   **Feature Importance:**  PyCaret can provide insights into which features are most important for predicting fraud. This can help you understand the underlying patterns.\n",
       "    *   **Threshold Tuning:**  The default threshold for classifying a transaction as fraudulent might not be optimal. Experiment with different thresholds to balance precision and recall based on your business needs.\n",
       "\n",
       "5.  **Data Quality:** The synthetic nature of the dataset might limit the generalizability of the model.  If possible, evaluate the model on a real-world dataset.\n",
       "\n",
       "To provide a more definitive answer, I would need to see the output of the `evaluate_model` calls to get the actual accuracy, precision, and recall values.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Model Interpretation\")\n",
    "display(Markdown(result['interpretation']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6b576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
